# -*- coding: utf-8 -*-
"""Feature Scaling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lcgoQxHzD6OaHyUmDx7BMqDamVRwD7uK

Standardization vs Normalization
"""

import seaborn as sns

import random

import pandas as pd
import numpy as np

l=random.sample(range(10,30),10)
l1=random.sample(range(100,500),10)

df=pd.DataFrame({'Age':l, 'Salary':l1}, index=[i for i in range(0,10)])

df

sns.kdeplot(df['Age'])

#sns.displot(df['Salary'])
sns.kdeplot(data=df['Salary'], shade=True)



"""Normalization(Min-Max Normalization):
Here we scale down the values of the features between 0 and 1.

X(norm) = (X-X(min))/(X(max)-X(min))




"""

from sklearn.preprocessing import MinMaxScaler

scaling=MinMaxScaler()

scaling.fit_transform(df[['Age','Salary']])

"""Standardization (Z-score normalization)

z=(x-μ)/σ
The scaling so happened would have transformed dataset in a way that mean =0 and std. deviation = 1 
"""

from sklearn.preprocessing import StandardScaler

scaling=StandardScaler()

scaling.fit_transform(df[['Age','Salary']])

"""Euclidean distance and gradient descent techniques
knn, k-means clustering, Linear and Logistic Regression etc use feature scaling
WHILE 
XG Boost, Decisition Tree, Random Forset---> they do not need scaling
"""